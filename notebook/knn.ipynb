{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "840\n",
      "(4200, 784)\n",
      "k = 1\n",
      "k = 5\n",
      "k = 25\n",
      "k = 125\n",
      "k = 625\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import defaultdict\n",
    "import math\n",
    "\n",
    "\n",
    "K_VALS = [1, 5, 25, 125, 625]\n",
    "PERCENT_VALIDATION = 0.2\n",
    "BANDWIDTHS = [10000, 50000, 100000, 1000000]\n",
    "\n",
    "\n",
    "def main():\n",
    "    df_train = pd.read_csv(\"train.csv\").sample(frac=0.1)\n",
    "    training_data = df_train.drop(\"label\", axis=1).values\n",
    "    training_classifications = df_train[\"label\"].values\n",
    "    validation_set_size = int(training_data.shape[0] * PERCENT_VALIDATION)\n",
    "    print(str(validation_set_size))\n",
    "    print(str(training_data.shape))\n",
    "\n",
    "    validation_errors = []\n",
    "    for i in range(len(K_VALS)):\n",
    "        print('k = ' + str(K_VALS[i]))\n",
    "        k = K_VALS[i]\n",
    "\n",
    "        # build the validation set\n",
    "        start_index = validation_set_size * i\n",
    "        end_index = validation_set_size * (i + 1)\n",
    "\n",
    "        validation_data = training_data[start_index:end_index]\n",
    "        validation_classifications = training_classifications[start_index:end_index]\n",
    "\n",
    "        # build the model\n",
    "        model = np.concatenate((training_data[:start_index], training_data[end_index:]), axis=0)\n",
    "        model_classifications = np.concatenate((training_classifications[:start_index], training_classifications[end_index:]), axis=0)\n",
    "\n",
    "        validation_error = get_validation_error(validation_data, validation_classifications, model, model_classifications, k)\n",
    "        validation_errors.append(validation_error)\n",
    "\n",
    "    plt.plot(K_VALS, validation_errors)\n",
    "    plt.title(\"K vs. Classification Error\")\n",
    "    plt.xlabel(\"k value\")\n",
    "    plt.xscale('log')\n",
    "    plt.ylabel(\"classification error\")\n",
    "    plt.savefig('k-nn.png')\n",
    "    plt.clf()\n",
    "    \n",
    "    validation_errors = []\n",
    "    # Kernalize with the Gaussian kernel over all the points\n",
    "    for i in range(len(BANDWIDTHS)):\n",
    "        print(\"bandwidth:\", str(BANDWIDTHS[i]))\n",
    "        bandwidth = BANDWIDTHS[i]\n",
    "        \n",
    "        # build the validation set\n",
    "        start_index = validation_set_size * i\n",
    "        end_index = validation_set_size * (i + 1)\n",
    "\n",
    "        validation_data = training_data[start_index:end_index]\n",
    "        validation_classifications = training_classifications[start_index:end_index]\n",
    "\n",
    "        # build the model\n",
    "        model = np.concatenate((training_data[:start_index], training_data[end_index:]), axis=0)\n",
    "        model_classifications = np.concatenate((training_classifications[:start_index], training_classifications[end_index:]), axis=0)\n",
    "        \n",
    "        validation_error = get_validation_error_gaussian(validation_data, validation_classifications, model, model_classifications, bandwidth)\n",
    "        validation_errors.append(validation_error)\n",
    "        \n",
    "    plt.plot(BANDWIDTHS, validation_errors)\n",
    "    plt.title(\"Bandwidth vs. Classification Error\")\n",
    "    plt.xlabel(\"k value\")\n",
    "    plt.xscale('log')\n",
    "    plt.ylabel(\"classification error\")\n",
    "    plt.savefig('k-nn-gaussian.png')\n",
    "    plt.clf()\n",
    "\n",
    "# @param: point1, point2 - arrays of pixel data for two points\n",
    "# @return: euclidean distance between the two points\n",
    "# Note: features are unweighted\n",
    "def get_distance(point1, point2):\n",
    "    assert(len(point1) == len(point2))\n",
    "    distance = math.sqrt(\n",
    "        np.sum(np.square([point1[i] - point2[i] for i in range(len(point1))])))\n",
    "    return distance\n",
    "\n",
    "def get_validation_error_gaussian(validation_block, validation_results, model, model_classifications, bandwidth):\n",
    "    misclassified = 0\n",
    "    for i in range(len(validation_block)):\n",
    "        point = validation_block[i]\n",
    "        prediction = make_prediction_gaussian(point, model, model_classifications, bandwidth)\n",
    "        if prediction != validation_results[i]:\n",
    "            misclassified += 1\n",
    "    return misclassified / len(validation_block)\n",
    "\n",
    "def make_prediction_gaussian(point, model, model_classifications, bandwidth):\n",
    "    classToKernelizedDistances = {}\n",
    "    denominator = 0\n",
    "    numerator = 0\n",
    "    for i in range(len(model)):\n",
    "        distance = get_distance(point, model[i])\n",
    "        #distance /= 1000 # Scale distance to avoid overflow\n",
    "        # Use the gaussian kernel\n",
    "        kernelizedDistance = math.exp(-1 * distance * distance/bandwidth)\n",
    "        denominator += kernelizedDistance\n",
    "        classification = model_classifications[i]\n",
    "        numerator += kernelizedDistance * classification   \n",
    "    return int(round(numerator/denominator))        \n",
    "\n",
    "# @param: point - array of pixel data from MNIST dataset relating to query point image\n",
    "# @param: data - the dataset to finding the k closest neighbors from\n",
    "# @param: k - number of numbers to find\n",
    "# @return: an array holding the k nearest neighbors of the query point, an array holding the respective distances\n",
    "def get_knn(point, data, k):\n",
    "    neighbors_and_dists = []  # array holding indexes of the k closest neighbors\n",
    "\n",
    "    for i in range(k):\n",
    "        neighbors_and_dists.append((i, np.linalg.norm(point - data[i])))\n",
    "    neighbors_and_dists.sort(key=lambda tup: tup[1])\n",
    "    #print(str(neighbors_and_dists))\n",
    "    for i in range(k + 1, len(data)):\n",
    "        dist = np.linalg.norm(point - data[i])\n",
    "        if dist < neighbors_and_dists[k - 1][1]:\n",
    "            search_index = k - 2\n",
    "            while search_index >= 0 and neighbors_and_dists[search_index][1] > dist:\n",
    "                search_index -= 1\n",
    "\n",
    "            neighbors_and_dists.insert(search_index + 1, (i, dist))\n",
    "\n",
    "    return list(neighbors_and_dists[:k])\n",
    "\n",
    "\n",
    "# @param: neighbors - array of indices of points\n",
    "# @param: results - array of output classifications\n",
    "# @param: distances - array of distances of each respective point in neighbors from a particular point\n",
    "# @return: a prediction of classification based off the classification with the lowest average distance from a point\n",
    "def make_prediction(neighbors_and_dists, classifications):\n",
    "    prediction_map = defaultdict(list)\n",
    "    for neighbor, distance in neighbors_and_dists:\n",
    "        neighbor_prediction = classifications[neighbor]\n",
    "\n",
    "        # Create a dictionary relating each possible prediction to a list of distances from query point\n",
    "        prediction_map[neighbor_prediction].append(distance)\n",
    "\n",
    "    # Relate each prediction to an average distance (among the neighbors)\n",
    "    for prediction in prediction_map:\n",
    "        prediction_map[prediction] = sum(prediction_map[prediction]) / float(len(prediction_map[prediction]))\n",
    "\n",
    "    return min(prediction_map, key=prediction_map.get)\n",
    "\n",
    "\n",
    "# @param: validationBlock - array of arrays holding point data for the validation set\n",
    "# @param: validationResults - array of integers corresponding to the respective classifications of the validation block\n",
    "# @param: nonValidationBlock - array of arrays holding point data for the training set\n",
    "# @param: nonValidationResults - array of integers corresponding to the respective classifications of the training block\n",
    "# @return: the ratio of incorrectly classified numbers in the validation set\n",
    "def get_validation_error(validation_block, validation_results, non_validation_block, non_validation_classifications, k):\n",
    "    misclassified = 0.0\n",
    "    for i in range(len(validation_block)):\n",
    "        point = validation_block[i]\n",
    "        neighbors_and_dists = get_knn(point, non_validation_block, k)\n",
    "        prediction = make_prediction(neighbors_and_dists, non_validation_classifications)\n",
    "        if prediction != validation_results[i]:\n",
    "            misclassified += 1\n",
    "    return misclassified / len(validation_block)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like the baseline we found, through cross validation, that a k of 1 to be best for our k-NN regression. For kernel regression, we found a bandwidth of 10^4 was best for our Nadaraya-Watson Gaussian kernel regression. Let's find out our test error with a model that is 1/4 of the training set for both algorithms.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest neighbor error is 0.045\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"../data/mnist_train.csv\").sample(frac=0.25)\n",
    "df_train['intercept'] = 1\n",
    "trainingData = df_train.drop(\"label\", axis = 1).values\n",
    "trainingResults = df_train[\"label\"].values\n",
    "df_test = pd.read_csv(\"../data/mnist_test.csv\")\n",
    "df_test['intercept'] = 1\n",
    "testData = df_test.drop(\"label\", axis=1).values\n",
    "testResults = df_test[\"label\"].values\n",
    "\n",
    "print(\"Nearest neighbor error is\", get_validation_error(testData, testResults, trainingData, trainingResults, 1))\n",
    "print(\"Gaussian kernel regression error is\", get_validation_error_gaussian(testData, testResults, trainingData, trainingResults, 10000))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
